{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "filepath='data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lens Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lens fluxes\n",
      "Loading background fluxes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11891, 13966, 3600, (25857,), (25857, 3600))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take in Raw data \n",
    "with open(f'{filepath}lens_flux-3600.data', 'rb') as filehandle:\n",
    "    print(\"Loading lens fluxes\")\n",
    "    # read the data as binary data stream\n",
    "    lens_flux = np.asarray(pickle.load(filehandle))\n",
    "    \n",
    "with open(f'{filepath}bg_flux-3600.data', 'rb') as filehandle:\n",
    "    print(\"Loading background fluxes\")\n",
    "    # read the data as binary data stream\n",
    "    bg_flux = np.asarray(pickle.load(filehandle))\n",
    "    \n",
    "nlenses,  nxlen  = lens_flux.shape\n",
    "nbg,  nxlen  = bg_flux.shape\n",
    "\n",
    "input_data = np.concatenate([\n",
    "    lens_flux, \n",
    "    bg_flux])\n",
    "\n",
    "labels = np.concatenate([np.full(nlenses, 0),\n",
    "                        np.full(nbg, 1)])\n",
    "\n",
    "nlenses, nbg, nxlen, labels.shape, input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size: 36, Number of Patches: 100, NumPatch/SpecSize: 0.01\n",
      "New Spectra Length:  3600\n",
      "Input Data Shape:  (25857, 3600)\n",
      "Labels Shape:  (25857,)\n"
     ]
    }
   ],
   "source": [
    "# Extract length of spectra\n",
    "spectra_length: int = input_data.shape[1]\n",
    "\n",
    "# Find lenght of patch ~ 1/100 of spectra length\n",
    "patch_size = spectra_length // 100 \n",
    "patch_num = spectra_length // patch_size \n",
    "new_spectra_length = patch_size * patch_num\n",
    "print(f\"Patch size: {patch_size}, Number of Patches: {patch_num}, NumPatch/SpecSize: {patch_size/new_spectra_length}\")\n",
    "print(\"New Spectra Length: \", new_spectra_length)\n",
    "# Pad spectra to be divisible by patch size\n",
    "input_data = np.pad(input_data, ((0,0), (0, new_spectra_length - spectra_length)), mode='constant')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Input Data Shape: \", input_data.shape)\n",
    "print(\"Labels Shape: \", labels.shape)\n",
    "\n",
    "\n",
    "# update spectra length\n",
    "spectra_length = input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  torch.Size([15514, 1, 3600])\n",
      "Test set:  torch.Size([5172, 1, 3600])\n",
      "Validation set:  torch.Size([5171, 1, 3600])\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_data, labels, test_size=1-0.6)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=.2/(.2+.2))\n",
    "\n",
    "# convert to torch tensors\n",
    "x_train = torch.Tensor(np.asarray(x_train).reshape(-1, 1, spectra_length))\n",
    "x_test = torch.Tensor(np.asarray(x_test).reshape(-1, 1, spectra_length))\n",
    "x_val = torch.Tensor(np.asarray(x_val).reshape(-1,1,spectra_length))\n",
    "y_train = torch.Tensor(y_train).long().squeeze()\n",
    "y_test = torch.Tensor(y_test).long().squeeze()\n",
    "y_val = torch.Tensor(y_val).long().squeeze()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training set: \", x_train.shape)\n",
    "print(\"Test set: \", x_test.shape)\n",
    "print(\"Validation set: \", x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all Training\n",
    "np.save('split/V1_xtrain.npy', x_train.cpu())\n",
    "np.save('split/V1_ytrain.npy', y_train.cpu())\n",
    "# Save Testing\n",
    "np.save('split/V1_xtest.npy', x_test.cpu())\n",
    "np.save('split/V1_ytest.npy', y_test.cpu())\n",
    "# Save Validation\n",
    "np.save('split/V1_xval.npy', x_val.cpu())\n",
    "np.save('split/V1_yval.npy', y_val.cpu())\n",
    "\n",
    "# Save all hyperparameters\n",
    "parameters = json.dumps({'patch_size':patch_size, 'patch_num':patch_num, 'spectra_length':spectra_length})\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "with open(\"split/parameters.json\",\"w\") as f:\n",
    "    f.write(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -sf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.9.1)",
   "language": "python",
   "name": "pytorch-1.9.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
