{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7af6f6e",
   "metadata": {},
   "source": [
    "# Classification of Simulated Lenses in DESI using Visual Transformer\n",
    "\n",
    "Author: Anthony LaBarca, modified by Delaney Cummins\n",
    "\n",
    "Date: 2023-07-24\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "License: ---\n",
    "\n",
    "Description: This script will adapt the ViT model to DESI spectral data of resolution 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731c2247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import torchsummary\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, \"__file__\")\n",
    "\n",
    "\n",
    "if is_interactive():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm, trange\n",
    "\n",
    "# Imports\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Synthetic Data Location \n",
    "#filepath to pickles\n",
    "# filepath='/global/homes/a/alabarca/DESI-Timedomain/simulated_preprocessing/old/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e822f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ViT Code (Don't want to deal with any imports atm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afae9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "EXAMPLE OUTPUT OF TRANFORMER WITH 100 PATCHES\n",
    "torch.Size([1, 100, 36])                                                                 # Original \n",
    "torch.Size([1, 100, 25])                                                                 # Move to hidden Dimension\n",
    "torch.Size([1, 101, 25])                                                                 # Add Classification Token\n",
    "torch.Size([1, 101, 25])                                                                 # Add positional embeddings\n",
    "torch.Size([1, 101, 25])                                                                 # 1 ViT Block\n",
    "torch.Size([1, 101, 25])                                                                 # 2 ViT Block\n",
    "torch.Size([1, 101, 25])                                                                 # 3 Vit Block\n",
    "torch.Size([1, 101, 25])                                                                 # 4 ViT Block\n",
    "torch.Size([1, 101, 25])                                                                 # 5 ViT Block\n",
    "torch.Size([1, 25])                                                                      # Take only the classification Token\n",
    "tensor([[0.4370, 0.1115, 0.0393, 0.0505, 0.0449, 0.3168]], grad_fn=<SoftmaxBackward>)    # Final Output (Predictions)\n",
    "'''\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b514a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f74980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ViT Training\n",
      "--------------------\n",
      "DEVICE IN USE cuda\n",
      "Model name:  vit_model_lens-3600-resample5x-50k\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing ViT Training\")\n",
    "print(\"-\"* 20)\n",
    "batch_size=256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"DEVICE IN USE {device}\")\n",
    "\n",
    "num_epochs= 60\n",
    "learning_rate=0.001\n",
    "model_suff='lens-3600-resample5x-50k'\n",
    "plot=False\n",
    "retrain = True\n",
    "continue_train = True\n",
    "previous_epoch = 0\n",
    "\n",
    " # model_name\n",
    "model_name = f\"vit_model_{model_suff}\"\n",
    "print(\"Model name: \", model_name)\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc204fc",
   "metadata": {},
   "source": [
    "### Import Data and turn into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d167025",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  torch.Size([15514, 1, 3600])\n",
      "Test set:  torch.Size([5172, 1, 3600])\n"
     ]
    }
   ],
   "source": [
    "# Load parameters and data\n",
    "with open(f\"split/parameters.json\", \"r\") as f:\n",
    "    model_params = json.load(f)\n",
    "\n",
    "patch_size = model_params['patch_size']\n",
    "patch_num = model_params['patch_num']\n",
    "spectra_length = model_params['spectra_length']\n",
    "\n",
    "x_train = torch.Tensor(np.load('split/V1_xtrain.npy'))\n",
    "y_train = torch.Tensor(np.load('split/V1_ytrain.npy')).long().squeeze()\n",
    "x_test = torch.Tensor(np.load('split/V1_xtest.npy'))\n",
    "y_test = torch.Tensor(np.load('split/V1_ytest.npy')).long().squeeze()\n",
    "\n",
    "print(\"Training set: \", x_train.shape)\n",
    "print(\"Test set: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6a3640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "def get_dataloaders(x_train, x_test, y_train, y_test, batch_size, device):\n",
    "    class SpectraDataset(Dataset):\n",
    "        def __init__(self, x, y):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.x)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx]\n",
    "    \n",
    "    train_dataset = SpectraDataset(x_train, y_train)\n",
    "    test_dataset = SpectraDataset(x_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=device)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(x_train, x_test, y_train, y_test, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542684ac",
   "metadata": {},
   "source": [
    "### Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb4a5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "def patchify(spectra: torch.Tensor, n_patches: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    spectra: 1D spectra: torch.Tensor of shape (N, 1, len_spectrum)\n",
    "    n_patches: number of patches to break the spectra into (must be a factor of len_spectrum)\n",
    "\n",
    "    return: patches of the spectra: torch.Tensor of shape (N, n_patches, len_spectrum // n_patches)\n",
    "    \"\"\"\n",
    "\n",
    "    n, _, l_spectrum = spectra.shape\n",
    "\n",
    "    # create patches\n",
    "    patch_size = l_spectrum // n_patches\n",
    "    patches = torch.zeros(n, n_patches, l_spectrum // n_patches)\n",
    "    for idx, spectrum in enumerate(spectra):\n",
    "        for i in range(n_patches):\n",
    "            patch = spectrum[:, i * patch_size: (i + 1) * patch_size]\n",
    "            patches[idx, i] = patch\n",
    "\n",
    "    return patches\n",
    "\n",
    "def positional_embedding(i, j, d):\n",
    "    \"\"\"\n",
    "    i: tensor index\n",
    "    j: embedding dimension\n",
    "\n",
    "    return: positional embedding for i, j\n",
    "    \"\"\"\n",
    "\n",
    "    if j % 2 == 0:\n",
    "        return np.sin(i / (10000 ** (j / d)))\n",
    "    return np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "\n",
    "\n",
    "def get_positional_embeddings(sequence_length: int, d) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    sequence_length: length of sequence\n",
    "    d: embedding dimension\n",
    "\n",
    "    return: positional embeddings for sequence of length sequence_length\n",
    "    \"\"\"\n",
    "\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = positional_embedding(i, j, d)\n",
    "\n",
    "    return result\n",
    "    \n",
    "class MSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        assert d % n_heads == 0, f\"Cannot divide dimension {d} into {n_heads} heads\"\n",
    "        \n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList(\n",
    "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, sequences):\n",
    "        \"\"\"\n",
    "        Sequences have shapes (N, seq_length, token_dim)\n",
    "        We must transform to shape (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        and concatenate back into (N, seq_length, token_dim)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "                \n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "                \n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_d, mlp_ratio * hidden_d), nn.GELU(),\n",
    "                                 nn.Linear(mlp_ratio * hidden_d, hidden_d))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Encoder1 will normalize input, pass through MSA,\n",
    "        add residual connection\n",
    "\n",
    "        Encoder2 will normalize encoder1, pass through MLP\n",
    "        \"\"\"\n",
    "        encoder1 = x + self.mhsa(self.norm1(x))\n",
    "        encoder2 = encoder1 + self.mlp(self.norm2(encoder1))\n",
    "        return encoder2\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, cl=(1, 1024), n_patches=64, n_blocks=2, hidden_d=8, n_heads=2, out_d=10, device = None):\n",
    "        super(ViT, self).__init__()\n",
    "        \n",
    "        # If device is provided, use that. \n",
    "        # HOwever, if CUDA is availible, that is the default device\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"ViT IS NOW IN {self.device}\")\n",
    "        \n",
    "        self.cl = cl  # (channels, length)\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        \n",
    "        assert cl[1] % n_patches == 0, \"Image length must be divisible by n_patches\"\n",
    "        self.patch_size = cl[1] // n_patches\n",
    "        \n",
    "        # Linear mapping of patches to hidden dimension\n",
    "        self.input_d = int(cl[0] * self.patch_size)\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d).to(self.device)\n",
    "        \n",
    "        # Classification Token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d).to(self.device))\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.pos_embed = nn.Parameter(get_positional_embeddings(\n",
    "            n_patches + 1, self.hidden_d).clone().detach())\n",
    "        self.pos_embed.requires_grad = False\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [ViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]).to(self.device)\n",
    "        \n",
    "        # Classification mlp\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1)).to(self.device)\n",
    "        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # Creating patches\n",
    "        n, _, _ = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(self.device)\n",
    "        # Linear tokenization --> map vector corresponding to each patch to hidden dimension\n",
    "        image_tokens = self.linear_mapper(patches)\n",
    "        # Adding classification\n",
    "        tokens = torch.stack([torch.vstack(\n",
    "            (self.class_token, image_tokens[i])) for i in range(len(image_tokens))])\n",
    "        # Adding positional embeddings\n",
    "        pos_embed = self.pos_embed.repeat(n, 1, 1)\n",
    "        out = tokens + pos_embed\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        \n",
    "        # For classification, we take the first token\n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out)\n",
    "    \n",
    "    def saveparams(self, model_name):\n",
    "        dict = {'cl': self.cl, 'patches':self.n_patches, 'n_blocks':self.n_blocks, 'n_heads':self.n_heads, 'hidden_d':self.hidden_d}\n",
    "        with open(f'{model_name}_parameters.json', 'w') as f:\n",
    "            json.dump(dict, f)\n",
    "\n",
    "# Create Old Model \n",
    "# model = ViT(cl=(1, spectra_length), n_patches=patch_num, n_blocks=4, hidden_d = patch_num // 4, n_heads = 5, out_d = 6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af3fe00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT IS NOW IN cuda\n"
     ]
    }
   ],
   "source": [
    "# Create New Model\n",
    "model = ViT(cl=(1, spectra_length), n_patches=patch_num, n_blocks=4, hidden_d = patch_size * 2, n_heads = 12, out_d = 2).to(device)\n",
    "\n",
    "model.saveparams(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer and Loss Functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5) \n",
    "\n",
    "# Initialize History Tracking\n",
    "model_history = {}\n",
    "# if continue_train and previous_epoch > 0:\n",
    "#     with open(f'{model_name}_history.json', \"r\") as f:\n",
    "#         model_history = json.load(f)\n",
    "#         print(\"Loaded\")\n",
    "#     model.load_state_dict(torch.load(f'model/{model_name}_epoch{previous_epoch}.pt'))\n",
    "#     print(\"Loaded\")\n",
    "\n",
    "#torchsummary.summary(model, (1, spectra_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "177871ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, device, epoch, criterion, optimizer):\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "        x, y = batch\n",
    "        x = x.type(torch.LongTensor) \n",
    "        y = y.type(torch.LongTensor) \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss\n",
    "\n",
    "def test_model(test_loader: DataLoader, model: nn.Module, device: torch.device, criterion, plot: bool = True):\n",
    "    \"\"\"\n",
    "    Test the model on the test set \n",
    "\n",
    "    Parameters: \n",
    "    -----------\n",
    "    test_loader: DataLoader \n",
    "    \"\"\"\n",
    "    model.train().to(device)\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x, y = batch\n",
    "        x = x.type(torch.LongTensor) \n",
    "        y = y.type(torch.LongTensor) \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        # force printout\n",
    "        loss = criterion(y_hat, y)\n",
    "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "        \n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1)\n",
    "                             == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "    # If you don't want Predictions, just return and get out \n",
    "    # if plot or y_hat is None:\n",
    "    return correct / total * 100, test_loss\n",
    "    \n",
    "#     # If you want a figure, make some predictions\n",
    "#     # get first batch from dataset, and plot the first 10 predictions made by the model\n",
    "#     data = next(iter(test_loader))\n",
    "#     x, y = data\n",
    "#     x = x.type(torch.LongTensor) \n",
    "#     y = y.type(torch.LongTensor) \n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     y_hat = model(x)\n",
    "\n",
    "#     fig, ax = plt.subplots(10, 1, figsize=(10, 20), sharex=\"all\")\n",
    "#     for i in range(10):\n",
    "#         print(f\"Prediction: {torch.argmax(y_hat[i])}, Label: {y[i]}\")\n",
    "#         ax[i].plot(x[i][0])\n",
    "#         ax[i].set_title(\n",
    "#             f\"Prediction: {torch.argmax(y_hat[i])}, Label: {y[i]}\")\n",
    "#     fig.supylabel(\"Intensity\")\n",
    "#     fig.supxlabel(\"Wavelength\")\n",
    "#     fig.tight_layout()\n",
    "#     return correct / total * 100, test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c0cfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5175b89dc28433d818dacdff9142979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 in training:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60 loss: 0.57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d42413e866487fb8fc7d69c83f6a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.53\n",
      "Test accuracy: 77.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e7594e57854dce854415afde34e738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 in training:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(num_epochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_epoch \u001b[38;5;241m+\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_epoch \u001b[38;5;241m+\u001b[39m num_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Test model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, device, epoch, criterion, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor) \n\u001b[1;32m      7\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n\u001b[1;32m     12\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 192\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    189\u001b[0m out \u001b[38;5;241m=\u001b[39m tokens \u001b[38;5;241m+\u001b[39m pos_embed\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 192\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# For classification, we take the first token\u001b[39;00m\n\u001b[1;32m    195\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 118\u001b[0m, in \u001b[0;36mViTBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Encoder1 will normalize input, pass through MSA,\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    add residual connection\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Encoder2 will normalize encoder1, pass through MLP\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     encoder1 \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmhsa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     encoder2 \u001b[38;5;241m=\u001b[39m encoder1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(encoder1))\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoder2\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mMSA.forward\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     84\u001b[0m v_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_mappings[head]\n\u001b[1;32m     86\u001b[0m seq \u001b[38;5;241m=\u001b[39m sequence[:, head \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head: (head \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head]\n\u001b[0;32m---> 87\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43mq_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m, k_mapping(seq), v_mapping(seq)\n\u001b[1;32m     89\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m     90\u001b[0m seq_result\u001b[38;5;241m.\u001b[39mappend(attention \u001b[38;5;241m@\u001b[39m v)\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_name) or retrain:\n",
    "    training_loss, test_loss, test_acc = [], [], []\n",
    "    if continue_train and previous_epoch > 0:\n",
    "        training_loss = model_history[\"training_loss\"]\n",
    "        test_loss = model_history[\"test_loss\"]\n",
    "        test_acc = model_history[\"test_acc\"]\n",
    "    print(len(training_loss))\n",
    "    # Training loop\n",
    "    for epoch in trange(num_epochs, desc=\"Training\"):\n",
    "        # Train model\n",
    "        model.train().to(device)\n",
    "\n",
    "        train_loss = train_model(train_loader, model, device, epoch, criterion, optimizer)\n",
    "        print(f\"Epoch {previous_epoch + epoch + 1}/{previous_epoch + num_epochs} loss: {train_loss:.2f}\")\n",
    "\n",
    "        # Test model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_acc_curr, test_loss_curr = test_model(test_loader, model, device, criterion, plot=False)\n",
    "        \n",
    "        scheduler.step(test_loss_curr)\n",
    "\n",
    "        # Bookkeeping\n",
    "        training_loss.append(train_loss)\n",
    "        test_loss.append(test_loss_curr)\n",
    "        test_acc.append(test_acc_curr)\n",
    "        # Save model\n",
    "        model_history = {\"training_loss\": training_loss, \"test_loss\": test_loss, \"test_acc\": test_acc}\n",
    "        # Save model and history\n",
    "        torch.save(model.state_dict(), f\"model/{model_name}_epoch{previous_epoch + epoch}.pt\")\n",
    "        \n",
    "        with open(f\"{model_name}_history.json\", \"w\") as f:\n",
    "            json.dump(model_history, f)\n",
    "\n",
    "    if plot:\n",
    "        with torch.no_grad():\n",
    "            test_model(test_loader, model, device, criterion, plot=plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ebd77",
   "metadata": {
    "tags": []
   },
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd5059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_history(history: dict, figure=None):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history: dict\n",
    "        Dictionary with training history\n",
    "    figure: tuple\n",
    "        Tuple of (fig, ax) to plot on\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig, ax\n",
    "        Figure and axis\n",
    "    \"\"\"\n",
    "    # plot on one axis, with two y axes (one for loss, one for accuracy)\n",
    "    \n",
    "    # figure is either None, or a tuple of (fig, ax)\n",
    "    if figure is None:\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=(10, 5))\n",
    "        assert isinstance(ax1, plt.Axes), \"Figure must be a tuple of (fig, ax)\"\n",
    "        ax2 = ax1.twinx()\n",
    "    else:\n",
    "        fig, ax = tuple(figure)\n",
    "        if isinstance(ax, np.ndarray):\n",
    "            ax1 = ax[0]\n",
    "            ax2 = ax[1]\n",
    "        else:\n",
    "            assert isinstance(\n",
    "                ax, plt.Axes), \"Figure must be a tuple of (fig, ax)\"\n",
    "            ax1 = ax\n",
    "            ax2 = ax1.twinx()\n",
    "    \n",
    "    assert isinstance(ax1, plt.Axes), \"Figure must be a tuple of (fig, ax)\"\n",
    "    assert isinstance(ax2, plt.Axes), \"Figure must be a tuple of (fig, ax)\"\n",
    "    \n",
    "    # List of colors to be used\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\",\n",
    "              \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.plot(history[\"training_loss\"], label=\"Training loss\", color=colors[0])\n",
    "    ax1.plot(history[\"test_loss\"], label=\"Test loss\", color=colors[1])\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    # Use the 3rd color in the color cycle\n",
    "    ax2.plot(history[\"test_acc\"], label=\"Test accuracy\", color=colors[2])\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.legend()\n",
    "    ax = (ax1, ax2)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36169e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "print(type(ax))\n",
    "history_figure2, hist_ax = plot_history(model_history, figure = (fig, ax))\n",
    "history_figure2.suptitle(\"Loss and Accuracy of ViT\")\n",
    "history_figure2.tight_layout()\n",
    "history_figure2.savefig(f'history-{model_name}.png', facecolor='white', transparent=False, edgecolor='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837e929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length: int, d) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    sequence_length: length of sequence\n",
    "    d: embedding dimension\n",
    "\n",
    "    return: positional embeddings for sequence of length sequence_length\n",
    "    \"\"\"\n",
    "    \n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = positional_embedding(i, j, d)\n",
    "    \n",
    "    return result\n",
    "\n",
    "test = nn.Parameter(get_positional_embeddings(100 + 1, 300).clone().detach())\n",
    "\n",
    "pos_embed = test.repeat(1, 1, 1)\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "print(pos_embed[0].detach().numpy().shape)\n",
    "ax.imshow(pos_embed[0].detach().numpy())\n",
    "ax.set_title(\"Positional Embeddings\")\n",
    "ax.set_xlabel(\"Hidden Dimension\")\n",
    "ax.set_ylabel(\"Patch #\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"embeddings.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79ef58-0020-4929-af72-17dd45453325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.13.1",
   "language": "python",
   "name": "pytorch-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
